{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering base on IR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirement\n",
    " - Window or Unix/Linux environment\n",
    " - Python 2.7\n",
    " - nltk\n",
    " - StanfordNERTagger : \n",
    "   - Download : http://nlp.stanford.edu/software/CRF-NER.shtml#Download\n",
    "   - Install JDK version >= 8\n",
    "   - Setting Environment variables for Window :\n",
    "     - Environment variable for CLASSPATH : path\\to\\stanford-ner\\stanford-ner.jar\n",
    "     - Environment variable for STANFORD_MODELS : path\\to\\stanford-ner\\classifiers\n",
    "   - Setting Environment variables for Unix : Open terminal type : gedit ~/.bashrc then move to the end of file and add :\n",
    "     - export CLASSPATH=path/to/stanford-ner/stanford-ner.jar\n",
    "     - export STANFORD_MODELS=path/to/stanford-ner/classifiers\n",
    "\n",
    " - sklearn\n",
    " - googleapiclient : pip install --upgrade google-api-python-client\n",
    " - BeautifulSoup\n",
    " - plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library & Setting Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import *\n",
    "from googleapiclient.discovery import build\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import timeit\n",
    "from collections import Counter\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import platform\n",
    "OS =  platform.system()\n",
    "if OS == 'Windows':\n",
    "    newline_character = '\\n'\n",
    "else:\n",
    "    newline_character = '\\r\\n'\n",
    "\n",
    "Seach_api_key = \"AIzaSyCYZt6vYMXhTn3dykAtVi6KrkQ1b30rd0c\"    #Change this key if Get Relevant Document step raise error\n",
    "Custom_Search_Engine_ID = \"005336700654283051786:1mzldt1husk\"\n",
    "num_pages = 30       #Number of retrieval page : 10,20,30,40,50,60,70.....\n",
    "\n",
    "# Setting environment variables before running this code\n",
    "ST3class = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')\n",
    "ST7class = StanfordNERTagger('english.muc.7class.distsim.crf.ser.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load trained AnswerType dection model\n",
    " - If this code raise error please run ATD_Tranning.ipynb again in AnswerTypeDection directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ATD_Function import getAnswerTypeIndex,clean_str,getPOSTag,feature_extraction,token\n",
    "\n",
    "answer_types = getAnswerTypeIndex('./AnswerTypeDetection/AnswerType.txt')\n",
    "ATD_Model = pickle.load(open('./AnswerTypeDetection/ATD_Model.pkl','rb'))\n",
    "Unigram_Vocabulary = pickle.load(open('./AnswerTypeDetection/Unigram_Vocabulary.pkl','rb'))\n",
    "UniPOS_List = pickle.load(open('./AnswerTypeDetection/UniPOS_List.pkl','rb'))\n",
    "CV_Unigram = CountVectorizer(vocabulary = Unigram_Vocabulary,ngram_range = (1,1))\n",
    "CV_UniPOS = CountVectorizer(vocabulary = UniPOS_List,ngram_range = (1,1),lowercase = False,tokenizer = token)\n",
    "f = open('./Data/Stopwords.txt')\n",
    "stopwords_list = f.read().split(newline_character)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load list enities and define entity tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem(sent,stemmer,decode = True):\n",
    "    try :\n",
    "        sent = sent.lower()\n",
    "        if decode:\n",
    "            sent = sent.decode('utf-8').strip()\n",
    "        words = sent.split(' ')\n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "        words = ' '.join(words)\n",
    "        return words\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def EntityTagger(sent,list_entity,stemmer):\n",
    "    tag_sent = pos_tag(word_tokenize(sent.lower()))\n",
    "    grammar = \"NP: {<DT>? <JJ>* <NN>*}\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    result = cp.parse(tag_sent)\n",
    "    NPs = []\n",
    "    for i in result:\n",
    "        if type(i) == Tree:\n",
    "            if i.label() == \"NP\":\n",
    "                text = \" \".join([token for token, pos in i.leaves()])\n",
    "                stem_text = stem(text,stemmer,decode = False)\n",
    "                if stem_text in list_entity:\n",
    "                    NPs.append(text)\n",
    "    return NPs\n",
    "\n",
    "Entities = ['ANIMAL','BODY','COLOR','CURRENCY','DISMED','FOOD','LANG','LETTER','PLANT','SPORT','VEHICLES']\n",
    "List_Enity = {}\n",
    "stemmer = PorterStemmer()\n",
    "for e in Entities:\n",
    "    f = open('Entity/' + e + '.txt','r')\n",
    "    enti = f.read().split(newline_character)\n",
    "    enti[0] = enti[0].replace('\\xef\\xbb\\xbf','')\n",
    "    enti = [stem(a,stemmer) for a in enti]\n",
    "    List_Enity[e] = enti\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Passage:\n",
    "    def __init__(self,string,rank,num_key,similar):\n",
    "        self.sent = string            #sentences\n",
    "        self.ner_tag = []             #named entities tag corresponding\n",
    "        self.num_ner = 0              #number of entities match answer-type\n",
    "        self.num_key = num_key        #number of keywords\n",
    "        self.len_long_seq = 0         #length of longest exact sequence of question keywords\n",
    "        self.rank = rank              #rank of own document\n",
    "        self.kw_similar = similar\n",
    "        self.ngram_overlap = 0        #ngram overlap question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That is everything this system require . Time to do experiments : \n",
    "\n",
    "The QA system are able to answer all below type of question :\n",
    " - PERSON\n",
    " - LOCATION\n",
    " - ORGANIZATION\n",
    " - MONEY\n",
    " - PERCENT\n",
    " - DATE\n",
    " - TIME\n",
    " - ANIMAL\n",
    " - BODY\n",
    " - COLOR\n",
    " - CURRENCY\n",
    " - DISEASE & MEDICINE\n",
    " - FOOD\n",
    " - LANGUAGE\n",
    " - LETTER\n",
    " - PLANT\n",
    " - SPORT\n",
    " - VEHICLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"What is the largest animal in the world ?\"                      #Change this\n",
    "question = query.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnswerType Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Feature Extraction\n",
    "num_question = feature_extraction([clean_str(question)],CV_Unigram,CV_UniPOS)\n",
    "AnswerTypeIdx = ATD_Model.predict(num_question)[0]\n",
    "AnswerType = answer_types[AnswerTypeIdx]\n",
    "print query\n",
    "print 'AnswerType : ' + AnswerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords Selection\n",
    "\n",
    " - Remove all word from English stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def keywords_selection(question, stopwords_list):\n",
    "    question = clean_str(question)\n",
    "    words = nltk.word_tokenize(question)\n",
    "    keywords = []\n",
    "    for w in words:\n",
    "        if w not in stopwords_list:\n",
    "            keywords.append(w)\n",
    "    return keywords\n",
    "\n",
    "question_keywords = keywords_selection(question,stopwords_list)\n",
    "print query\n",
    "print 'Keywords : ' + ' - '.join(question_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Relevant Document\n",
    " - Using google seach API to get relevant document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "service = build(\"customsearch\", \"v1\",\n",
    "            developerKey=Seach_api_key)\n",
    "\n",
    "pages_content = []\n",
    "for i in range(0, int((float(num_pages)/10))):\n",
    "    if (i == 0):\n",
    "        res = service.cse().list(q=question,cx = Custom_Search_Engine_ID).execute()\n",
    "    else:\n",
    "        res = service.cse().list(q=question,cx = Custom_Search_Engine_ID,num=10,start = i*10).execute()\n",
    "    pages_content += res[u'items']\n",
    "\n",
    "document_urls = []\n",
    "document_titles = []\n",
    "for page in pages_content:\n",
    "    if 'fileFormat' in page:\n",
    "        print 'Skip ' +  page[u'link']\n",
    "        continue\n",
    "    document_urls.append(page[u'link'])\n",
    "    document_titles.append(page[u'title'])\n",
    "    \n",
    "for i in range(0,len(document_urls)):\n",
    "    print document_titles[i]\n",
    "    print document_urls[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passage Retrieval\n",
    " - Get all sentences from all document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_num_keyword(sent,keywords,stemmer):\n",
    "    stem_sent = stem(sent,stemmer,decode = False)\n",
    "    num_key = 0\n",
    "    for kw in keywords:\n",
    "        if stemmer.stem(kw) in stem_sent:\n",
    "            num_key += 1\n",
    "    return num_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all candidate passages from all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all candidate passages from all documents\n",
    "passages = []\n",
    "combine_all_sent = \"\"          #Combine all passages to speed up NER tagger\n",
    "total_start = timeit.default_timer()\n",
    "for i in range(0,len(document_urls)):\n",
    "    start = timeit.default_timer()\n",
    "    try:\n",
    "        html = requests.get(document_urls[i], timeout = 5)\n",
    "    except:\n",
    "        print 'Cannot read ' + document_urls[i]\n",
    "        continue\n",
    "    stop = timeit.default_timer()\n",
    "    tree = BeautifulSoup(html.text,'lxml')\n",
    "    print 'Analyzing ' + document_urls[i] + ' : ' + str(round(stop - start,2)) + 's'\n",
    "    # Remove invisible elements\n",
    "    for invisible_elem in tree.find_all(['script', 'style']):\n",
    "        invisible_elem.extract()\n",
    "    sents = nltk.sent_tokenize(tree.get_text())\n",
    "    for sent in sents:\n",
    "        for sub_sent in sent.split('\\n'):\n",
    "            sub_sent = sub_sent.strip()\n",
    "            if (len(sub_sent) > 0 and len(sub_sent) < 1000):\n",
    "                num_keyword = get_num_keyword(sub_sent,question_keywords,stemmer)\n",
    "                if (num_keyword > 0):\n",
    "                    passages.append(Passage(sub_sent,i,num_keyword,0))\n",
    "                    combine_all_sent += sub_sent + \" Endofsent \"\n",
    "                \n",
    "total_end = timeit.default_timer()\n",
    "print 'Time elapse : ' + str(round(total_end - total_start,2)) + 's'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagging name enity for each passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "if (AnswerTypeIdx >= 7):\n",
    "    for p in passages:\n",
    "        p.ner_tag = EntityTagger(p.sent,List_Enity[AnswerType],stemmer)\n",
    "else:\n",
    "    if (AnswerType == \"PERSON\" or AnswerType == \"LOCATION\" or AnswerType == \"ORGANIZATION\"):\n",
    "        classified_text = ST3class.tag(word_tokenize(combine_all_sent))\n",
    "    else:\n",
    "        classified_text = ST7class.tag(word_tokenize(combine_all_sent))\n",
    "    i = 0\n",
    "    words = []\n",
    "    for t in classified_text:\n",
    "        if (t[0] == 'Endofsent'):\n",
    "            i += 1\n",
    "            continue\n",
    "        if (t[1] == AnswerType):\n",
    "            words.append(t[0])\n",
    "        else:\n",
    "            if (len(words) > 0 ):\n",
    "                passages[i].ner_tag.append(' '.join(words))\n",
    "                words = []\n",
    "                \n",
    "stop = timeit.default_timer()\n",
    "print 'Time elapse : ' + str(stop - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminate passages have no entity match answer type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def entity_filter(tags,question):\n",
    "    if len(tags) == 0:\n",
    "        return False\n",
    "    for t in tags:\n",
    "        if t.lower() not in question:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "print 'Total number of passages : ' + str(len(passages))\n",
    "passages = [p for p in passages if entity_filter(p.ner_tag,question)]\n",
    "print 'After Filtering : ' + str(len(passages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Passages by number of keyword\n",
    "\n",
    " - Find the maximum number of question keyword contain in a passages\n",
    " - Keep passages have number of question keyword < MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Total passages : ' +  str(len(passages))\n",
    "max_keyword = 0\n",
    "min_num_passages = 20\n",
    "for p in passages:\n",
    "    if p.num_key > max_keyword:\n",
    "        max_keyword = p.num_key\n",
    "    \n",
    "while (True):\n",
    "    num_candidate_passages = 0\n",
    "    for p in passages:\n",
    "        if p.num_key >= max_keyword:\n",
    "            num_candidate_passages += 1\n",
    "    if (num_candidate_passages >= min_num_passages or max_keyword == 1):\n",
    "        break\n",
    "    else:\n",
    "        max_keyword -=1 \n",
    "print 'Max number of question keyword : ' + str(max_keyword)\n",
    "passages = [p for p in passages if p.num_key >= max_keyword]\n",
    "print 'After filtering : ' +  str(len(passages)) + '\\n'\n",
    "for i in range(0,min(10,len(passages))):\n",
    "    print str(i) + ' - ' + passages[i].sent + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Extracting\n",
    "\n",
    " - Correct answer is the entity with highest frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def contain(s1,s2,stemmer):\n",
    "    s1 = stemmer.stem(s1.lower())\n",
    "    s2 = stemmer.stem(s2.lower())\n",
    "    s1 = s1.split()\n",
    "    s2 = s2.split()\n",
    "    for w in s1:\n",
    "        if w not in s2:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def get_most_common(answers):\n",
    "    answers = Counter(answers).most_common()\n",
    "    for i in range(0,len(answers)):\n",
    "        if type(answers[i]) != tuple:\n",
    "            continue\n",
    "        full_answer = ''\n",
    "        for j in range(i + 1,len(answers)):\n",
    "            if type(answers[j]) != tuple:\n",
    "                continue\n",
    "            if contain(answers[i][0],answers[j][0],stemmer) and full_answer =='' and answers[j][1] > 1:\n",
    "                full_answer = answers[j][0]\n",
    "            if contain(answers[i][0],answers[j][0],stemmer) or contain(answers[j][0],answers[i][0],stemmer):\n",
    "                answers[i] = (answers[i][0],answers[i][1] + answers[j][1])\n",
    "                answers[j] = -1\n",
    "        if full_answer != '':\n",
    "            answers[i] = (full_answer,answers[i][1])\n",
    "            \n",
    "    answers = [a for a in answers if type(a)== tuple] \n",
    "    answers = (sorted(answers, key=lambda tup: tup[1]))\n",
    "    answers.reverse()\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candidates_answer = []\n",
    "for p in passages:\n",
    "    candidates_answer += p.ner_tag\n",
    "candidates_answer = [a for a in candidates_answer if a.lower() not in question]\n",
    "final_answers = get_most_common(candidates_answer)\n",
    "final_answers = final_answers[0 : min(7,len(final_answers))]\n",
    "print '\\nThe final answer is : ' + final_answers[0][0]\n",
    "names = [a[0] for a in final_answers]\n",
    "names.reverse()\n",
    "freqs = [a[1] for a in final_answers]\n",
    "freqs.reverse()\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "py.offline.init_notebook_mode()\n",
    "data = [go.Bar(\n",
    "            x=freqs,\n",
    "            y=names,\n",
    "            orientation = 'h'\n",
    ")]\n",
    "layout = go.Layout(margin=dict(l=150,r=10,t=10,b=80))\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
