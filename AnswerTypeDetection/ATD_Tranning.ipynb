{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Answer-Type index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f = open('.\\Data\\ATD_train.txt')\n",
    "import platform\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getAnswerTypeIndex(filename):\n",
    "    f = open(filename)\n",
    "    if platform.system() == 'Windows':\n",
    "        lines = f.read().split('\\n')\n",
    "    else:\n",
    "        lines = f.read().split('\\r\\n')\n",
    "    answer_types = {}\n",
    "    for line in lines:\n",
    "        answer_types[line.split(' ')[1]] = int(line.split(' ')[0])\n",
    "        answer_types[int(line.split(' ')[0])] = line.split(' ')[1]\n",
    "    f.close()\n",
    "    return answer_types\n",
    "\n",
    "answer_types = getAnswerTypeIndex('AnswerType.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get train-question and its type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!\\']\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()\n",
    "\n",
    "def getPOSTag(sentences):\n",
    "    tags = nltk.pos_tag(nltk.word_tokenize(sentences))\n",
    "    tags_sequence = \"\"\n",
    "    for tag in tags:\n",
    "        tags_sequence += tag[1] + \" \"\n",
    "    return tags_sequence\n",
    "def token(s):\n",
    "    return s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WP NN VBD DT NN IN DT JJ NN IN DT NNP \n"
     ]
    }
   ],
   "source": [
    "newline = ''\n",
    "if platform.system() == 'Windows':\n",
    "    newline = '\\n'\n",
    "else:\n",
    "    newline = '\\r\\n'\n",
    "f = open('./Data/ATD_train.txt')\n",
    "lines = f.read().replace('\\xef\\xbb\\xbf','').split(newline)\n",
    "questions = []\n",
    "questions_POSTag = []\n",
    "questions_type = []\n",
    "\n",
    "for line in lines:\n",
    "    qt = line[0 : line.index(' ')]\n",
    "    q = line[line.index(' ') + 1 : -1]\n",
    "    questions.append(clean_str(q))\n",
    "    questions_POSTag.append(getPOSTag(q))\n",
    "    questions_type.append(answer_types[qt])\n",
    "print questions_POSTag[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Vocabulary for Unigram and UniPOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv_Word = CountVectorizer(ngram_range = (1,1))\n",
    "arr_features = cv_Word.fit_transform(questions).toarray()\n",
    "\n",
    "cv_Pos = CountVectorizer(lowercase = False,tokenizer = token, ngram_range = (1,1))\n",
    "array_Pos = cv_Pos.fit_transform(questions_POSTag).toarray()\n",
    "\n",
    "# Save Vocabulary and POS\n",
    "pickle.dump(cv_Word.vocabulary_,open('Unigram_Vocabulary.pkl', 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(cv_Pos.vocabulary_,open('UniPOS_List.pkl','wb'),pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def feature_extraction(question,CV_Unigram,CV_UniPOS):\n",
    "    if (type(question) is str):\n",
    "        question_POSTag = getPOSTag(question)\n",
    "        arr_feature = CV_Unigram.transform(question).toarray()\n",
    "        arr_feature += CV_UniPOS.transform(question_POSTag).toarray()\n",
    "    else:\n",
    "        question_POSTag = [getPOSTag(q) for q in question]\n",
    "        arr_feature = np.array(CV_Unigram.transform(question).toarray())\n",
    "        arr_feature = np.hstack((arr_feature,np.array(CV_UniPOS.transform(question_POSTag).toarray())))\n",
    "    return np.array(arr_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_voca = pickle.load(open('Unigram_Vocabulary.pkl','rb'))\n",
    "pos_list = pickle.load(open('UniPOS_List.pkl','rb'))\n",
    "\n",
    "CV_Unigram = CountVectorizer(vocabulary = word_voca, ngram_range = (1,1))\n",
    "CV_UniPOS = CountVectorizer(vocabulary = pos_list,lowercase = False,tokenizer = token,ngram_range = (1,1))\n",
    "\n",
    "train_question = feature_extraction(questions,CV_Unigram,CV_UniPOS)\n",
    "train_label = questions_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.svm as SVM\n",
    "model = SVM.LinearSVC()\n",
    "model.fit(train_question,train_label)\n",
    "\n",
    "#Saving model \n",
    "pickle.dump(model,open('ATD_Model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9137254901960784"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "f = open('./Data/ATD_test.txt')\n",
    "lines = f.read().replace('\\xef\\xbb\\xbf','').split(newline)\n",
    "test_questions = []\n",
    "test_questions_type = []\n",
    "\n",
    "for line in lines:\n",
    "    qt = line[0 : line.index(' ')]\n",
    "    q = line[line.index(' ') + 1 : -1]\n",
    "    test_questions.append(clean_str(q))\n",
    "    test_questions_type.append(answer_types[qt])\n",
    "\n",
    "test_question = feature_extraction(test_questions,CV_Unigram,CV_UniPOS)\n",
    "test_label = test_questions_type\n",
    "\n",
    "model2 = pickle.load(open('ATD_Model.pkl','rb'))\n",
    "accuracy_score(test_label,model2.predict(test_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 75  0  0  0  3  0  1  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  1  1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 47  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2  0  2  0  0  0  0 10  0  0  0  2  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0]\n",
      " [ 0  3  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0]\n",
      " [ 2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "a = confusion_matrix(test_label, model2.predict(test_question))\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
